import os
import datetime
import multiprocessing
import time
import json
import re
import fsspec
import pprint
import dask
import subprocess
import pathlib
import glob
import itertools

import pandas as pd
import numpy as np
import xarray as xr

from shapely import geometry
from tqdm import tqdm
from p_tqdm import p_umap
from kerchunk.hdf import SingleHdf5ToZarr
from kerchunk.combine import MultiZarrToZarr
from dask.distributed import Client, LocalCluster

from .convenience import Convenience
from .metadata import Metadata
from .store import IPLD


class Creation(Convenience):
    """
    Base class for transforming a collection of downloaded input files in NetCDF4 Classic format into
    (sequentially) kerchunk JSONs, a MultiZarr Kerchunk JSON, and finally an Xarray Dataset based on that MultiZarr.
    """

    # KERCHUNKING

    def create_zarr_json(self, force_overwrite: bool = False):
        """
        Convert list of local input files (MultiZarr) to a single JSON representing a "virtual" Zarr

        Read each file in the local input directory and create an in-memory JSON object representing it as a Zarr,
        then read that collection of JSONs (MultiZarr) into one master JSON formatted as a Zarr and hence openable as a single file

        Note that MultiZarrToZarr will fail if chunk sizes are inconsistent due to inconsistently sized data inputs (e.g. different
        numbers of steps in input datasets)

        Parameters
        ----------
        force_overwrite : bool, optional
            Switch to create (or not) a new JSON at `DatasetManager.zarr_json_path()` even if the path exists

        """
        input_files_list = [
            str(fil)
            for fil in self.input_files()
            if (fil.suffix == ".nc4" or fil.suffix == ".nc")
        ]
        self.zarr_json_path().parent.mkdir(mode=0o755, exist_ok=True)
        # Generate a multizarr if it doesn't exist. If one exists, use that.
        if not self.zarr_json_path().exists() or force_overwrite:
            self.info(f"Generating Zarr for {self.file_type} files")
            start_kerchunking = time.time()
            self.info(
                f"Processing {len(input_files_list)} files with {multiprocessing.cpu_count()} processors"
            )
            zarr_jsons = list(map(self.kerchunkify, tqdm(input_files_list)))
            mzz = MultiZarrToZarr(zarr_jsons, **self.mzz_opts())
            mzz.translate(filename=self.zarr_json_path())
            self.info(
                f"Kerchunking to Zarr --- {round((time.time() - start_kerchunking)/60,2)} minutes"
            )
        else:
            self.info(f"Existing Zarr found, using that")

    def kerchunkify(self, input_file: str):
        """
        Transform input NetCDF into a JSON representing it as a Zarr. These JSONs can be merged into a MultiZarr that Xarray can open natively as a Zarr.

        NOTE under the hood there are several versions of GRIB files -- GRIB1 and GRIB2 -- and NetCDF files -- classic, netCDF-4 classic, 64-bit offset, etc.
        Kerchunk will fail on some versions in undocumented ways. We have found consistent success with netCDF-4 classic files so presuppose using those.

        The command line tool `nccopy -k 'netCDF-4 classic model' infile.nc outfile.nc` can convert between formats

        Parameters
        ----------
        input_file : str
            A file path to a NetCDF-4 Classic file

        """
        fs = fsspec.filesystem("file")
        fs_nc = fs.open(input_file)
        with fs_nc as infile:
            try:
                return SingleHdf5ToZarr(infile, fs_nc.path).translate()
            except OSError as e:
                raise ValueError(
                    f"Error found with {fs_nc.path}, likely due to incomplete file. Full error message is {e}"
                )

    @classmethod
    def mzz_opts(cls) -> dict:
        """
        Class method to populate with options to be passed to MultiZarrToZarr.
        The options dict is by default populated with class variables instantiated above;
        optional additional parameters can be added as per the needs of the input dataset

        Returns
        -------
        opts : dict
            Kwargs for kerchunk's MultiZarrToZarr method
        """
        opts = dict(
            remote_protocol=cls.remote_protocol(),
            identical_dims=cls.identical_dims(),
            concat_dims=cls.concat_dims(),
            preprocess=cls.preprocess_kerchunk,
        )
        return opts

    # PRE AND POST PROCESSING

    @classmethod
    def preprocess_kerchunk(cls, refs: dict) -> dict:
        """
        Class method to populate with the specific preprocessing routine of each child class (if relevant), whilst the file is being read by Kerchunk.
        Note this function usually works by manipulating Kerchunk's internal "refs" -- the zarr dictionary generated by Kerchunk.

        If no preprocessing is happening, return the dataset untouched

        Parameters
        ----------
        refs : dict
            Dataset attributes and information automatically supplied by Kerchunk

        Returns
        -------
        refs : dict
            Dataset attributes and information, transformed as needed

        """
        ref_names = set()
        file_match_pattern = "(.*?)/"
        for ref in refs:
            if re.match(file_match_pattern, ref) is not None:
                ref_names.add(re.match(file_match_pattern, ref).group(1))
        for ref in ref_names:
            fill_value_fix = json.loads(refs[f"{ref}/.zarray"])
            fill_value_fix["fill_value"] = str(cls.missing_value_indicator())
            refs[f"{ref}/.zarray"] = json.dumps(fill_value_fix)
        return refs

    def postprocess_zarr(self, dataset: xr.Dataset) -> xr.Dataset:
        """
        Method to populate with the specific postprocessing routine of each child class (if relevant)

        If no preprocessing is happening, return the dataset untouched

        Parameters
        ----------
        dataset : xr.Dataset
            The dataset being processed

        Returns
        -------
        dataset : xr.Dataset
            The dataset being processed

        """
        return dataset

    # CONVERT FILES

    def convert_to_lowest_common_time_denom(
        self, raw_files: list, keep_originals: bool = False
    ):
        """
        Decompose a set of raw files aggregated by week, month, year, or other irregular time denominator
        into a set of smaller files, one per the lowest common time denominator -- hour, day, etc.

        Parameters
        ----------
        raw_files : list
            A list of file path strings referencing the original files prior to processing
        originals_dir : pathlib.Path
            A path to a directory to hold the original files
        keep_originals : bool, optional
            An optional flag to preserve the original files for debugging purposes. Defaults to False.
        """
        if raw_files:
            originals_dir = pathlib.Path(raw_files[0]).parent.parent / (
                pathlib.Path(raw_files[0]).parent.stem + "_originals"
            )
            for raw_file in raw_files:
                command_args = [
                    "cdo",
                    "-f",
                    "nc4",
                    "splitsel,1",
                    raw_file,
                    os.path.splitext(raw_file)[0],
                ]  # subprocess CDO doesn't like pathlib paths, must use strings
                subprocess.run(command_args, check=True)
                raw_file = pathlib.Path(raw_file)
                if keep_originals:
                    originals_dir.mkdir(mode=0o755, parents=True, exist_ok=True)
                    raw_file.rename(originals_dir / raw_file.name)
                else:
                    raw_file.unlink()

    def ncs_to_nc4s(self, keep_originals: bool = False):
        """
        Find all NetCDF files in the input folder and batch convert them
        in parallel to NetCDF4-Classic files that play nicely with Kerchunk

        NOTE There are many versions of NetCDFs and some others seem to play nicely with Kerchunk.
        NOTE To be safe we convert to NetCDF4 Classic as these are reliable and no data is lost.

        Parameters
        ----------
        keep_originals : bool
            A flag to preserve the original files for debugging purposes.
        """
        input_dir = pathlib.Path(self.local_input_path())
        # Build a series of (filename, keep_originals) tuples to pass in parallel to p_map
        raw_files = glob.glob(str(input_dir / "*.nc"))
        job_args = [(raw_file, keep_originals) for raw_file in raw_files]
        # convert raw NetCDFs to NetCDF4-Classics in parallel
        self.info(
            f"Converting {(len(list(raw_files)))} NetCDFs to NetCDF4 Classic files"
        )
        p_umap(self.sb_nc_copy, job_args)
        self.info(
            f"{(len(list(raw_files)))} conversions finished"
        )

    @staticmethod
    def sb_nc_copy(job_args):
        """
        Static method to convert an input NetCDF file into a NetCDF4 file.
        Meant to be used in parallel w/ arguments provided as an iterable.
        Optionally moves the original file to a "<dataset_name>_originals" folder for reference

        Parameters
        ----------
        job_args : tuple
            An iterable of (str, bool) elements indicating the file path and whether to keep the original file.
        """
        # define file paths
        file, keep_originals = job_args[0], job_args[1]
        file = pathlib.Path(file)
        originals_dir = file.parents[1] / (file.parent.stem + "_originals")
        # set up and run conversion subprocess on command line
        subprocess_args = [
            "nccopy",
            "-k",
            "netCDF-4 classic model",
            file,
            file.with_suffix(".nc4"),
        ]
        subprocess.run(subprocess_args, check=True)
        # keep or get rid of original files
        if keep_originals:
            pathlib.Path.mkdir(originals_dir, mode=0o755, parents=True, exist_ok=True)
            file.rename(originals_dir / file.name)
        else:
            file.unlink()

    # RETURN DATASET

    def zarr_hash_to_dataset(self, ipfs_hash: str) -> xr.Dataset:
        """
        Open a Zarr on IPLD at `ipfs_hash` as an `xr.Dataset` object

        Parameters
        ----------
        ipfs_hash : str
            The CID of the dataset

        Returns
        -------
        dataset : xr.Dataset
            Object representing the dataset described by the CID at `self.latest_hash()`

        """
        mapper = self.store.mapper(set_root=False)
        mapper.set_root(ipfs_hash)
        dataset = xr.open_zarr(mapper)
        return dataset

    def zarr_json_to_dataset(self) -> xr.Dataset:
        """
        Open the virtual zarr at `self.zarr_json_path()` and return as a xr.Dataset object

        Returns
        -------
        xr.Dataset
            Object representing the dataset described by the Zarr JSON file at `self.zarr_json_path()`

        """
        dataset = xr.open_dataset(
            "reference://",
            engine="zarr",
            chunks={},
            backend_kwargs={
                "storage_options": {
                    "fo": str(self.zarr_json_path()),
                    "remote_protocol": self.remote_protocol(),
                    "skip_instance_cache": True,
                    "default_cache_type": "readahead",
                },
                "consolidated": False,
            },
        )
        # Apply any further postprocessing on the way out
        return self.postprocess_zarr(dataset)


class Publish(Creation, Metadata):
    """
    Base class for publishing methods -- both initial publication and updates
    """

    # PARSING

    def parse(self, *args, **kwargs) -> bool:
        """
        Open all raw files in self.local_input_path(). Transform the data contained in them into Zarr format and write to the store specified
        by `Attributes.store`.

        If the store is IPLD or S3, an existing Zarr will be searched for to be opened and appended to by default. This can be overridden to force
        writing the entire input data to a new Zarr by setting `Convenience.rebuild_requested` to `True`. If existing data is found,
        `DatasetManager.overwrite_allowed` must also be `True`.

        This is the core function for transforming and writing data (to disk, S3, or IPLD) and should be standard for all ETLs. Modify the
        child methods it calls or the dask configuration settings to resolve any performance or parsing issues.

        Parameters
        ----------
        args : list
            Additional arguments passed from generate.py
        kwargs : dict
            Keyword arguments passed from generate.py

        Returns
        -------
        bool
            Flag indicating if new data was / was not parsed

        """
        self.info("Running parse routine")
        # adjust default dask configuration parameters as needed
        self.dask_configuration()
        # Use a Dask client to open, process, and write the data
        with LocalCluster(
            processes=False,
            dashboard_address="127.0.0.1:8787",  # specify local IP to prevent exposing the dashboard
            protocol="inproc://",  # otherwise Dask may default to tcp or tls protocols and choke
            threads_per_worker=self.dask_num_threads,
            n_workers=self.dask_num_workers,
        ) as cluster, Client(
            cluster,
        ) as client:
            try:
                # Attempt to find an existing Zarr, using the appropriate method for the store. If there is existing data and there is no
                # rebuild requested, start an update. If there is no existing data, start an initial parse. If rebuild is requested and there is
                # no existing data or allow overwrite has been set, write a new Zarr, overwriting (or in the case of IPLD, not using) any existing
                # data. If rebuild is requested and there is existing data, but allow overwrite is not set, do not start parsing and issue a warning.
                if self.store.has_existing and not self.rebuild_requested:
                    self.info(f"Updating existing data at {self.store}")
                    self.update_zarr()
                elif not self.store.has_existing or (
                    self.rebuild_requested and self.overwrite_allowed
                ):
                    if not self.store.has_existing:
                        self.info(
                            f"No existing data found. Creating new Zarr at {self.store}."
                        )
                    else:
                        self.info(f"Data at {self.store} will be replaced.")
                    self.write_initial_zarr()
                else:
                    self.zarr_to_ipld()
            except KeyboardInterrupt:
                self.info(
                    "CTRL-C Keyboard Interrupt detected, exiting Dask client before script terminates"
                )
                client.close()

        return True

    def to_zarr(self, dataset: xr.Dataset, *args, **kwargs):
        """
        Wrapper around `xr.Dataset.to_zarr`. `*args` and `**kwargs` are forwarded to `to_zarr`. The dataset to write to Zarr must be the first argument.

        On S3 and local, pre and post update metadata edits are saved to the Zarr attrs at `Dataset.update_in_progress` to indicate during writing that
        the data is being edited.

        Parameters
        ----------
        dataset
            Dataset to write to Zarr format
        *args
            Arguments to forward to `xr.Dataset.to_zarr`
        **kwargs
            Keyword arguments to forward to `xr.Dataset.to_zarr`
        """
        # Skip update in-progress metadata flag on IPLD
        if not isinstance(self.store, IPLD):
            # Create an empty dataset that will be used to just write the metadata (there's probably a better way to do this? compute=False?).
            dataset.attrs["update_in_progress"] = True
            empty_dataset = dataset
            for coord in itertools.chain(dataset.coords, dataset.data_vars):
                empty_dataset = empty_dataset.drop(coord)

            # If there is an existing Zarr, indicate in the metadata that an update is in progress, and write the metadata before starting the real write.
            if self.store.has_existing:
                self.info("Pre-writing metadata to indicate an update is in progress")
                empty_dataset.to_zarr(
                    self.store.mapper(refresh=True), append_dim="time"
                )

        # Write data to Zarr and log duration.
        start_writing = time.perf_counter()
        dataset.to_zarr(*args, **kwargs)
        self.info(
            f"Writing Zarr took {datetime.timedelta(seconds=time.perf_counter() - start_writing)}"
        )

        # Skip update in-progress metadata flag on IPLD
        if not isinstance(self.store, IPLD):
            # Indicate in metadata that update is complete.
            empty_dataset.attrs["update_in_progress"] = False
            self.info(
                "Re-writing Zarr to indicate in the metadata that update is no longer in process."
            )
            empty_dataset.to_zarr(self.store.mapper(), append_dim="time")

    # SETUP

    def dask_configuration(self):
        """
        Convenience method to implement changes to the configuration of the dask client after instantiation

        NOTE Some relevant paramters and console print statements we found useful during testing have been left
        commented out at the bottom of this function. Consider activating them if you encounter trouble parsing
        """
        self.info("Configuring Dask")
        dask.config.set(
            {
                "distributed.scheduler.worker-saturation": self.dask_scheduler_worker_saturation
            }
        )  # toggle upwards or downwards (minimum 1.0) depending on memory mgmt performance
        dask.config.set(
            {"distributed.scheduler.worker-ttl": None}
        )  # will timeout on big tasks otherwise
        dask.config.set(
            {"distributed.worker.memory.target": self.dask_worker_mem_target}
        )
        dask.config.set({"distributed.worker.memory.spill": self.dask_worker_mem_spill})
        dask.config.set({"distributed.worker.memory.pause": self.dask_worker_mem_pause})
        dask.config.set(
            {"distributed.worker.memory.terminate": self.dask_worker_mem_terminate}
        )

        # OTHER USEFUL SETTINGS, USE IF ENCOUNTERING PROBLEMS WITH PARSES
        # dask.config.set({'scheduler' : 'threads'}) # default distributed scheduler does not allocate memory correctly for some parses
        # dask.config.set({'nanny.environ.pre-spawn-environ.MALLOC_TRIM_THRESHOLD_' : 0}) # helps clear out unused memory
        # dask.config.set({"distributed.worker.memory.recent-to-old-time": "300s"}) #???

        # DEBUGGING
        self.info(f"dask.config.config is {pprint.pformat(dask.config.config)}")

    # INITIAL PUBLICATION

    def pre_initial_dataset(self) -> xr.Dataset:
        """
        Get an `xr.Dataset` that can be passed to the appropriate writing method when writing a new Zarr. Read the virtual Zarr JSON at the
        path returned by `Creation.zarr_json_path`, normalize the axes, re-chunk the dataset according to this object's chunking parameters, and
        add custom metadata defined by this class.

        Returns
        -------
        xr.Dataset
            The dataset from `Creation.zarr_json_to_dataset` with custom metadata, normalized axes, and rechunked
        """
        # Transform the JSON Zarr into an xarray Dataset
        dataset = self.zarr_json_to_dataset()

        # Reset standard_dims to Arbol's standard now that loading + preprocessing on the original names is done
        self.standard_dims = ["latitude", "longitude", "time"]
        dataset = dataset.transpose("time", "latitude", "longitude")

        # Re-chunk
        self.info(f"Re-chunking dataset to {self.requested_dask_chunks}")
        dataset = dataset.chunk(self.requested_dask_chunks)
        self.info(f"Chunks after rechunk are {dataset.chunks}")

        # Add metadata to dataset
        dataset = self.set_zarr_metadata(dataset)

        # Log the state of the dataset before writing
        self.info(f"Initial dataset\n{dataset}")

        return dataset

    def write_initial_zarr(self):
        """
        Writes the first iteration of zarr for the dataset to the store specified at
        initialization. If the store is `IPLD`, does some additional metadata processing
        """
        # Transform the JSON Zar
        dataset = self.pre_initial_dataset()
        mapper = self.store.mapper(set_root=False)

        self.to_zarr(dataset, mapper, consolidated=True, mode="w")

        if isinstance(self.store, IPLD):
            self.dataset_hash = str(mapper.freeze())
            self.info(f"IPFS hash is {str(self.dataset_hash)}")

            # Create and publish metadata as a STAC Item
            self.create_root_stac_catalog()
            self.create_stac_collection(dataset)
            self.create_stac_item(dataset, self.dataset_hash)
            self.info("Published dataset's IPFS hash is " + str(self.dataset_hash))

    # UPDATES

    def update_zarr(self):
        """
        Update discrete regions of an N-D dataset saved to disk as a Zarr. If updates span multiple date ranges, pushes separate updates to each region.
        If the IPLD store is in use, after updating the dataset, this function updates the corresponding STAC Item and summaries in the parent
        STAC Collection.
        """
        original_dataset = self.store.dataset()
        update_dataset = self.zarr_json_to_dataset()

        # reset standard_dims to Arbol's standard now that loading + preprocessing on the original names is done
        self.standard_dims = ["latitude", "longitude", "time"]

        self.info(f"Original dataset\n{original_dataset}")

        # Prepare inputs for the update operation
        insert_times, append_times = self.update_setup(original_dataset, update_dataset)

        # Conduct update operations
        self.update_parse_operations(
            original_dataset, update_dataset, insert_times, append_times
        )

    def update_setup(
        self, original_dataset: xr.Dataset, update_dataset: xr.Dataset
    ) -> tuple[list, list]:
        """
        Create needed inputs for the actual update parses: a variable to hold the hash and lists of any times to insert and/or append.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing xr.Dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records

        Returns
        -------
        insert_times : list
            Datetimes corresponding to existing records to be replaced in the original dataset
        append_times : list
            Datetimes corresponding to all new records to append to the original dataset
        """
        # Create a static variable to hold the hash of the updated dataset for IPLD store
        self.dataset_hash = None
        original_times = set(original_dataset.time.values)
        if (
            type(update_dataset.time.values) == np.datetime64
        ):  # cannot perform iterative (set) operations on a single numpy.datetime64 value
            update_times = set([update_dataset.time.values])
        else:  # many values will come as an iterable numpy.ndarray
            update_times = set(update_dataset.time.values)
        insert_times = sorted(update_times.intersection(original_times))
        append_times = sorted(update_times - original_times)

        return insert_times, append_times

    def update_parse_operations(
        self,
        original_dataset: xr.Dataset,
        update_dataset: xr.Dataset,
        insert_times: list,
        append_times: list,
    ):
        """
        An enclosing method triggering insert and/or append operations based on the presence of valid records for either.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        insert_times : list
            Datetimes corresponding to existing records to be replaced in the original dataset
        append_times : list
            Datetimes corresponding to all new records to append to the original dataset
        """
        # Raise an exception if there is no writable data
        if not insert_times and not append_times:
            raise ValueError(
                "Update started with no new records to insert or append to original zarr."
            )

        original_chunks = {
            dim: val_tuple[0] for dim, val_tuple in original_dataset.chunks.items()
        }
        # First write out updates to existing data using the 'region=' command...
        if len(insert_times) > 0:
            if not self.overwrite_allowed:
                self.warn(
                    "Not inserting records despite historical data detected. 'allow_overwrite'"
                    "flag has not been set and store is not IPLD"
                )
            else:
                self.insert_into_dataset(
                    original_dataset, update_dataset, insert_times, original_chunks
                )
        else:
            self.info("No modified records to insert into original zarr")
        # ...then write new data (appends) using the 'append_dim=' command
        if len(append_times) > 0:
            self.append_to_dataset(update_dataset, append_times, original_chunks)
        else:
            self.info("No new records to append to original zarr")

        # In the case of IPLD store, update STAC
        if isinstance(self.store, IPLD):
            # Regenerate the STAC Item and update the relevant Collection entry with the final hash
            self.finalize_update_metadata()

    def insert_into_dataset(
        self,
        original_dataset: xr.Dataset,
        update_dataset: xr.Dataset,
        insert_times: list,
        original_chunks: list,
    ):
        """
        Insert new records to an existing dataset along its time dimension using the `append_dim=` flag.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing xr.Dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        insert_times : list
            Datetimes corresponding to existing records to be replaced in the original dataset
        originaL_chunks : dict
            A Dict containing the dimension:size parameters for the original dataset
        """
        mapper = self.store.mapper()

        insert_dataset = self.prep_update_dataset(
            update_dataset, insert_times, original_chunks
        )
        date_ranges, regions = self.calculate_update_time_ranges(
            original_dataset, insert_dataset
        )
        for dates, region in zip(date_ranges, regions):
            insert_slice = insert_dataset.sel(time=slice(dates[0], dates[1]))
            insert_dataset.attrs["update_is_append_only"] = False
            self.info("Indicating the dataset is not appending data only.")
            self.to_zarr(
                insert_slice.drop(self.standard_dims[:2]),
                mapper,
                region={"time": slice(region[0], region[1])},
            )

        self.info(
            f"Inserted records for {len(insert_dataset.time.values)} times from {len(regions)} date range(s) to original zarr"
        )
        # In the case of IPLD, store the hash for later use
        if isinstance(self.store, IPLD):
            self.dataset_hash = str(mapper.freeze())

    def append_to_dataset(
        self, update_dataset: xr.Dataset, append_times: list, original_chunks: dict
    ):
        """
        Append new records to an existing dataset along its time dimension using the `append_dim=` flag.

        Parameters
        ----------
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        append_times : list
            Datetimes corresponding to all new records to append to the original dataset
        originaL_chunks : dict
            The dimension:size parameters for the original dataset
        """
        append_dataset = self.prep_update_dataset(
            update_dataset, append_times, original_chunks
        )
        mapper = self.store.mapper()

        # Write the Zarr
        append_dataset.attrs["update_is_append_only"] = True
        self.info("Indicating the dataset is appending data only.")
        self.to_zarr(append_dataset, mapper, consolidated=True, append_dim="time")

        self.info(
            f"Appended records for {len(append_dataset.time.values)} datetimes to original zarr"
        )
        # In the case of IPLD, store the hash for later use
        if isinstance(self.store, IPLD):
            self.dataset_hash = str(mapper.freeze())

    def prep_update_dataset(
        self, update_dataset: xr.Dataset, time_filter_vals: list, new_chunks: dict
    ) -> xr.Dataset:
        """
        Select out and format time ranges you wish to insert or append into the original dataset based on specified time range(s) and chunks

        Parameters
        ----------
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        time_filter_vals : list
            Datetimes corresponding to all new records to insert or append
        new_chunks : dict
            A Dict containing the dimension:size parameters for the original dataset

        Returns
        -------
        update_dataset : xr.Dataset
            An xr.Dataset filtered to only the time values in `time_filter_vals`, with correct metadata
        """
        # Xarray will automatically drop dimensions of size 1. A missing time dimension causes all manner of update failures.
        if "time" in update_dataset.dims:
            update_dataset = update_dataset.sel(time=time_filter_vals).transpose(
                "time", self.standard_dims[0], self.standard_dims[1]
            )
        else:
            update_dataset = update_dataset.expand_dims("time").transpose(
                "time", self.standard_dims[0], self.standard_dims[1]
            )
        update_dataset = update_dataset.chunk(new_chunks)
        update_dataset = self.set_zarr_metadata(update_dataset)

        self.info(f"Update dataset\n{update_dataset}")

        return update_dataset

    def calculate_update_time_ranges(
        self, original_dataset: xr.Dataset, update_dataset: xr.Dataset
    ) -> tuple[list[datetime.datetime], list[str]]:
        """
        Calculate the start/end dates and index values for contiguous time ranges of updates.
        Used by `update_zarr` to specify the location(s) of updates in a target Zarr dataset.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing xr.Dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records

        Returns
        -------
        datetime_ranges : list[datetime.datetime, datetime.datetime]
            A List of (Datetime, Datetime) tuples defining the time ranges of records to insert
        regions_indices: list[int, int]
             A List of (int, int) tuples defining the indices of records to insert

        """
        dataset_time_span = f"1{self.temporal_resolution()[0]}"  # NOTE this won't work for months (returns 1 minute), we could define a more precise method with if/else statements if needed.
        complete_time_series = pd.Series(update_dataset.time.values)
        # Define datetime range starts as anything with > 1 unit diff with the previous value, and ends as > 1 unit diff with the following. First/Last will return NAs we must fill.
        starts = (complete_time_series - complete_time_series.shift(1)).abs().fillna(
            pd.Timedelta(dataset_time_span * 100)
        ) > pd.Timedelta(dataset_time_span)
        ends = (complete_time_series - complete_time_series.shift(-1)).abs().fillna(
            pd.Timedelta(dataset_time_span * 100)
        ) > pd.Timedelta(dataset_time_span)
        # Filter down the update time series to just the range starts/ends
        insert_datetimes = complete_time_series[starts + ends]
        single_datetime_inserts = complete_time_series[starts & ends]
        # Add single day insert datetimes once more so they can be represented as ranges, then sort for the correct order. Divide the result into a collection of start/end range arrays
        insert_datetimes = np.sort(
            pd.concat(
                [insert_datetimes, single_datetime_inserts], ignore_index=True
            ).values
        )
        datetime_ranges = np.array_split(insert_datetimes, (len(insert_datetimes) / 2))
        # Calculate a tuple of the start/end indices for each datetime range
        regions_indices = []
        for date_pair in datetime_ranges:
            start_int = list(original_dataset.time.values).index(
                original_dataset.sel(time=date_pair[0], method="nearest").time
            )
            end_int = (
                list(original_dataset.time.values).index(
                    original_dataset.sel(time=date_pair[1], method="nearest").time
                )
                + 1
            )
            regions_indices.append((start_int, end_int))

        return datetime_ranges, regions_indices

    def finalize_update_metadata(self):
        """
        Publish updated STAC Item and Collection metadata reflecting the new dataset hash and parameters
        """
        final_dataset = self.zarr_hash_to_dataset(self.dataset_hash)
        self.info(f"IPFS hash is {str(self.dataset_hash)}")
        self.create_stac_item(final_dataset, self.dataset_hash)
        self.update_stac_collection(final_dataset)
