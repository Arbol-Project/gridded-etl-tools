
import os, datetime, ipldstore, multiprocessing, multiprocessing.pool, time, \
    json, re, fsspec, pprint, ipldstore, dask, subprocess, pathlib, glob

import pandas as pd
import numpy as np
import xarray as xr

from shapely import geometry
from tqdm import tqdm
from p_tqdm import p_map
from kerchunk.hdf import SingleHdf5ToZarr
from kerchunk.combine import MultiZarrToZarr
from dask.distributed import Client, LocalCluster
from zarr.errors import ContainsGroupError

from etls.utils.convenience import Convenience
from etls.utils.metadata import Metadata

class Creation(Convenience):
    """
    Base class for transforming a collection of downloaded input files in NetCDF4 Classic format into
    (sequentially) kerchunk JSONs, a MultiZarr Kerchunk JSON, and finally an Xarray Dataset based on that MultiZarr.
    """


    # KERCHUNKING


    def create_zarr_json(self, force_overwrite: bool = False):
        """
        Convert list of local input files (MultiZarr) to a single JSON representing a "virtual" Zarr

        Read each file in the local input directory and create an in-memory JSON object representing it as a Zarr,
        then read that collection of JSONs (MultiZarr) into one master JSON formatted as a Zarr and hence openable as a single file

        Note that MultiZarrToZarr will fail if chunk sizes are inconsistent due to inconsistently sized data inputs (e.g. different
        numbers of steps in input datasets)

        Parameters
        ----------
        force_overwrite : bool, optional
            Switch to create (or not) a new JSON at `DatasetManager.zarr_json_path()` even if the path exists

        """
        input_files_list = [str(fil) for fil in self.input_files() if (fil.suffix == '.nc4' or fil.suffix == '.nc')]
        os.makedirs(self.zarr_json_path().parent, 0o755, True)
        # Generate a multizarr if it doesn't exist. Otherwise move ahead
        if not self.zarr_json_path().exists() or force_overwrite:
            self.info(f"Generating Zarr for {self.file_type} files")
            start_kerchunking = time.time()
            self.info(f"Processing {len(input_files_list)} files with {multiprocessing.cpu_count()} processors")
            zarr_jsons = list(map(self.kerchunkify, tqdm(input_files_list)))
            mzz = MultiZarrToZarr(zarr_jsons,  **self.mzz_opts())
            mzz.translate(filename=self.zarr_json_path())
            self.info(f"Kerchunking to Zarr --- {round((time.time() - start_kerchunking)/60,2)} minutes")


    def kerchunkify(self, input_file: str):
        """
        Transform input NetCDF into a JSON representing it as a Zarr. These JSONs can be merged into a MultiZarr that Xarray can open natively as a Zarr.

        NOTE under the hood there are several versions of GRIB files -- GRIB1 and GRIB2 -- and NetCDF files -- classic, netCDF-4 classic, 64-bit offset, etc.
        Kerchunk will fail on some versions in undocumented ways. We have found consistent success with netCDF-4 classic files so presuppose using those.
        
        The command line tool `nccopy -k 'netCDF-4 classic model' infile.nc outfile.nc` can convert between formats

        Parameters
        ----------
        input_file : str
            A file path to a NetCDF-4 Classic file

        """
        fs = fsspec.filesystem('file')
        fs_nc = fs.open(input_file)
        with fs_nc as infile:
            try:
                return SingleHdf5ToZarr(infile, fs_nc.path).translate()
            except OSError as e:
                raise ValueError(f"Error found with {fs_nc.path}, likely due to incomplete file. Full error message is {e}")


    @classmethod
    def mzz_opts(cls) -> dict:
        """
        Class method to populate with options to be passed to MultiZarrToZarr. 
        The options dict is by default populated with class variables instantiated above;
        optional additional parameters can be added as per the needs of the input dataset

        Returns
        -------
        opts : dict
            Kwargs for kerchunk's MultiZarrToZarr method
        """
        opts = dict(remote_protocol = cls.remote_protocol(),
                        identical_dims = cls.identical_dims(),
                        concat_dims = cls.concat_dims(),
                        preprocess = cls.preprocess_kerchunk)
        return opts


    # PRE AND POST PROCESSING


    @classmethod
    def preprocess_kerchunk(cls, refs: dict) -> dict:
        """
        Class method to populate with the specific preprocessing routine of each child class (if relevant), whilst the file is being read by Kerchunk.
        Note this function usually works by manipulating Kerchunk's internal "refs" -- the zarr dictionary generated by Kerchunk.

        If no preprocessing is happening, return the dataset untouched

        Parameters
        ----------
        refs : dict
            Dataset attributes and information automatically supplied by Kerchunk

        Returns
        -------
        refs : dict
            Dataset attributes and information, transformed as needed

        """
        ref_names = set()
        file_match_pattern = "(.*?)/"
        for ref in refs:
            if re.match(file_match_pattern, ref) is not None:
                ref_names.add(re.match(file_match_pattern, ref).group(1))
        for ref in ref_names:
            fill_value_fix = json.loads(refs[f'{ref}/.zarray'])
            fill_value_fix["fill_value"] = str(cls.missing_value_indicator())
            refs[f'{ref}/.zarray'] = json.dumps(fill_value_fix)
        return refs


    def postprocess_zarr(self, dataset: xr.Dataset) -> xr.Dataset:
        """
        Class method to populate with the specific postprocessing routine of each child class (if relevant)

        If no preprocessing is happening, return the dataset untouched

        Parameters
        ----------
        dataset : xr.Dataset
            The dataset being processed

        Returns
        -------
        dataset : xr.Dataset
            The dataset being processed

        """
        return dataset


    # CONVERT FILES


    def convert_to_lowest_common_time_denom(self, raw_files: list, keep_originals: bool = False):
        """
        Decompose a set of raw files aggregated by week, month, year, or other irregular time denominator 
        into a set of smaller files, one per the lowest common time denominator -- hour, day, etc.

        Parameters
        ----------
        raw_files : list
            A list of file path strings referencing the original files prior to processing
        originals_dir : pathlib.Path
            A path to a directory to hold the original files
        keep_originals : bool, optional
            An optional flag to preserve the original files for debugging purposes. Defaults to False.
        """
        originals_dir = pathlib.Path(raw_files[0]).parents[1] / (pathlib.Path(raw_files[0]).parent.stem + '_originals')
        for raw_file in raw_files:
            command_args = ["cdo", "-f", "nc4", "splitsel,1", raw_file, os.path.splitext(raw_file)[0]] # subprocess CDO doesn't like pathlib paths, must use strings
            subprocess.run(command_args, check=True)
            raw_file = pathlib.Path(raw_file)
            if keep_originals:
                originals_dir.mkdir(mode=0o755, parents=True, exist_ok=True)
                raw_file.rename(originals_dir / raw_file.name)
            else:
                raw_file.unlink()


    def ncs_to_nc4s(self, keep_originals: bool = False):
        """
        Find all NetCDF files in the input folder and batch convert them
        in parallel to NetCDF4-Classic files that play nicely with Kerchunk

        NOTE There are many versions of NetCDFs and some others seem to play nicely with Kerchunk.
        NOTE To be safe we convert to NetCDF4 Classic as these are reliable and no data is lost.

        Parameters
        ----------
        keep_originals : bool
            A flag to preserve the original files for debugging purposes.
        """
        input_dir = pathlib.Path(self.local_input_path())
        # Build a series of (filename, keep_originals) tuples to pass in parallel to p_map
        raw_files = glob.glob(str(input_dir / "*.nc"))
        job_args = [(raw_file, keep_originals) for raw_file in raw_files]
        # convert raw NetCDFs to NetCDF4-Classics in parallel
        self.info(f"Converting {(len(list(raw_files)))} NetCDFs to NetCDF4 Classic files")
        p_map(self.sb_nc_copy, job_args)


    @staticmethod
    def sb_nc_copy(job_args):
        """
        Static method to convert an input NetCDF file into a NetCDF4 file.
        Meant to be used in parallel w/ arguments provided as an iterable.
        Optionally moves the original file to a "<dataset_name>_originals" folder for reference

        Parameters
        ----------
        job_args : tuple
            An iterable of (str, bool) elements indicating the file path and whether to keep the original file.
        """
        # define file paths
        file, keep_originals = job_args[0], job_args[1]
        file = pathlib.Path(file)
        originals_dir = file.parents[1] / (file.parent.stem + '_originals')
        # set up and run conversion subprocess on command line
        subprocess_args = ["nccopy", "-k", "netCDF-4 classic model", file, file.with_suffix(".nc4")]
        subprocess.run(subprocess_args, check=True)
        # keep or get rid of original files
        if keep_originals:
            pathlib.Path.mkdir(originals_dir, mode=0o755, parents=True, exist_ok=True)
            file.rename(originals_dir / file.name)
        else:
            file.unlink()


    # RETURN DATASET


    def zarr_hash_to_dataset(self, ipfs_hash: str) -> xr.Dataset:
        """
        Open a zarr archived at self.latest_hash() as an xr.Dataset object

        Parameters
        ----------
        ipfs_hash : str
            The CID of the dataset being published

        Returns
        -------
        dataset : xr.Dataset
            Object representing the dataset described by the CID at `self.latest_hash()`

        """
        mapper = self.ipld_mapper()
        mapper.set_root(ipfs_hash)
        dataset = xr.open_zarr(mapper)
        return dataset


    def zarr_json_to_dataset(self) -> xr.Dataset:
        """
        Open the virtual zarr at `self.zarr_json_path()` and return as a xr.Dataset object

        Returns
        -------
        xr.Dataset 
            Object representing the dataset described by the Zarr JSON file at `self.zarr_json_path()`

        """
        dataset = xr.open_dataset(
            "reference://",
            engine="zarr",
            chunks={},
            backend_kwargs={
                "storage_options": {
                    "fo": str(self.zarr_json_path()),
                    "remote_protocol": self.remote_protocol(),
                    "skip_instance_cache": True,
                    "default_cache_type": "readahead"
                },
                "consolidated": False
            })
        # Apply any further postprocessing on the way out
        return self.postprocess_zarr(dataset)


class Publish(Creation, Metadata):
    """
    Base class for publishing methods -- both initial publication and updates
    """

    # PARSING

    def parse(self, local_output: bool = False, *args, **kwargs) -> bool:
        """
        Open all raw files in self.local_input_path(), transform the data into Zarr format and write to IPLD. If self.rebuild_requested
        is not set (the default), data on IPLD at self.latest_hash() will be opened and appended to. The new hash will be added to the heads
        file and returned as a string.

        This is the core function for transforming and writing data (to disk or IPLD) and should be standard for all ETLs. Modify the child methods
        it calls or the dask configuration settings to resolve any performance or parsing issues.

        Should return `True` if new data was parsed and `False` if not.

        Parameters
        ----------
        local_output : bool, optional
            Switch triggering the output of data locally, instead of to IPFS
        args : list
            Additional arguments passed from generate.py
        kwargs : dict
            Keyword arguments passed from generate.py

        Returns
        -------
        bool
            Flag indicating if new data was / was not parsed

        """
        self.info("Running parse routine")
        # Dynamically adjust metadata based on fields calculated during `update_local_input`, if necessary (usually not)
        self.populate_metadata()
        # Create 1 file per measurement span (hour, day, week, etc.) so Kerchunk has consistently chunked inputs for MultiZarring
        self.prepare_input_files()
        # Create Zarr JSON outside of Dask client so multiprocessing can use all workers / threads without interference from Dask
        self.create_zarr_json()
         # adjust default dask configuration parameters as needed
        self.dask_configuration()
        # Use a Dask client to open, process, and write the data
        with LocalCluster(processes=False,
                          dashboard_address="127.0.0.1:8787", # specify local IP to prevent exposing the dashboard
                          protocol="inproc://", # otherwise Dask may default to tcp or tls protocols and choke
                          threads_per_worker=self.dask_num_threads,
                          n_workers=self.dask_num_workers
        ) as cluster, Client(cluster, ) as client:
            try:
                self.update_zarr()
            except FileNotFoundError:
                if local_output:
                    self.zarr_to_disk()
                else:
                    self.zarr_to_ipld()
            except KeyboardInterrupt:
                self.info("CTRL-C Keyboard Interrupt detected, exiting Dask client before script terminates")
                client.close()

        return True

    # SETUP

    def dask_configuration(self):
        """
        Convenience method to implement changes to the configuration of the dask client after instantiation

        NOTE Some relevant paramters and console print statements we found useful during testing have been left
        commented out at the bottom of this function. Consider activating them if you encounter trouble parsing
        """
        self.info("Configuring Dask")
        dask.config.set({"distributed.scheduler.worker-saturation" : self.dask_scheduler_worker_saturation}) # toggle upwards or downwards (minimum 1.0) depending on memory mgmt performance
        dask.config.set({"distributed.scheduler.worker-ttl": None}) # will timeout on big tasks otherwise
        dask.config.set({"distributed.worker.memory.target": self.dask_worker_mem_target})
        dask.config.set({"distributed.worker.memory.spill": self.dask_worker_mem_spill})
        dask.config.set({"distributed.worker.memory.pause": self.dask_worker_mem_pause})
        dask.config.set({"distributed.worker.memory.terminate": self.dask_worker_mem_terminate})

        # OTHER USEFUL SETTINGS, USE IF ENCOUNTERING PROBLEMS WITH PARSES
        # dask.config.set({'scheduler' : 'threads'}) # default distributed scheduler does not allocate memory correctly for some parses
        # dask.config.set({'nanny.environ.pre-spawn-environ.MALLOC_TRIM_THRESHOLD_' : 0}) # helps clear out unused memory
        # dask.config.set({"distributed.worker.memory.recent-to-old-time": "300s"}) #???

        # DEBUGGING
        self.info(f"dask.config.config is {pprint.pformat(dask.config.config)}")

    # INITIAL PUBLICATION

    def zarr_to_disk(self):
        """
        Save the first version of a dataset to disk (instead of IPLD) as a Zarr. Used for debugging.
        """
        dataset = self.zarr_json_to_dataset()

        self.common_vars = ["latitude", "longitude", "time"] # reset common_vars to Arbol standard now that loading + preprocessing on the original names is done
        dataset = dataset.transpose("time", "latitude", "longitude")
        dataset = dataset.chunk(self.requested_dask_chunks)

        dataset = self.set_zarr_metadata(dataset)
 
        print("------------------------Dataset---------------------------")
        print(dataset)
        print("----------------------------------------------------------")

        try:
            dataset.to_zarr(self.output_path(), consolidated=True)
        except ContainsGroupError:
            self.info("Zarr already exists at local output path and Xarray cannot overwrite Zarrs. Please delete the existing Zarr and re-run.")


    def zarr_to_ipld(self):
        """
        Save the first version of a dataset to IPLD as a Zarr.
        """
        dataset = self.zarr_json_to_dataset()
        
        self.common_vars = ["latitude", "longitude", "time"] # reset common_vars to Arbol standard now that loading + preprocessing on the original names is done
        dataset = dataset.transpose("time", "latitude", "longitude")
        self.info(f"Re-chunking dataset to {self.requested_dask_chunks}")
        dataset = dataset.chunk(self.requested_dask_chunks)
        self.info(f"Chunks after rechunk are {dataset.chunks}")

        dataset = self.set_zarr_metadata(dataset)
        dataset = self.set_zarr_spatial_ref(dataset)
        
        print("------------------------Dataset---------------------------")
        print(dataset)
        print("----------------------------------------------------------")
        start_writing = time.time()
        # Parse the data to the server
        mapper = self.ipld_mapper()
        dataset.to_zarr(mapper, consolidated=True)
        self.info(f"Writing out Zarr --- {round((time.time() - start_writing)/3600,3)} hours")
        
        # Record the published dataset's IPFS hash
        self.dataset_hash = str(mapper.freeze())
        self.info(f"IPFS hash is {str(self.dataset_hash)}")
        # Create and publish metadata as a STAC Item
        self.create_root_stac_catalog()
        self.create_stac_collection(dataset)
        self.create_stac_item(dataset, self.dataset_hash)
        self.info("Published dataset's IPFS hash is " + str(self.dataset_hash))

    # UPDATES

    def update_zarr(self):
        """
        Update discrete regions of an N-D dataset saved to disk as a Zarr. If updates span multiple date ranges, pushes separate updates to each region.
        After updating the dataset, this function updates the corresponding STAC Item and summaries in the parent STAC Collection
        """
        if not self.rebuild_requested:
            if self.latest_hash():
                self.info(f"Found existing Zarr at {self.latest_hash()}")
                ...
            else:
                self.info(f"No Zarr found at {self.latest_hash()}, creating new one")
                raise FileNotFoundError
        else:
            self.info(f"Rebuild requested, creating new Zarr")
            raise FileNotFoundError

        original_dataset = self.zarr_hash_to_dataset(self.latest_hash())
        update_dataset = self.zarr_json_to_dataset()

        # reset common_vars to Arbol standard now that loading + preprocessing on the original names is done
        self.common_vars = ["latitude", "longitude", "time"]

        print("---------------------Original dataset---------------------")
        print(original_dataset)
        print("----------------------------------------------------------")

        # Prepare inputs for the update operation
        insert_times, append_times, ipld_ds_mapper = self.update_setup(original_dataset, update_dataset)
        # Conduct update operations
        self.update_parse_operations(original_dataset, update_dataset, insert_times, append_times, ipld_ds_mapper)


    def update_setup(self, original_dataset: xr.Dataset, update_dataset: xr.Dataset) -> tuple[list, list, ipldstore.IPLDStore]:
        """
        Create needed inputs for the actual update parses: a variable to hold the hash, a mapper to link it to an IPLD object,
        and lists of any times to insert and/or append.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing xr.Dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records

        Returns
        -------
        insert_times : list
            Datetimes corresponding to existing records to be replaced in the original dataset
        append_times : list
            Datetimes corresponding to all new records to append to the original dataset
        ipld_ds_mapper : ipldstore.IPLDStore
            An IPLDStore Mapper object linking the Zarr to a CID

        """
        # Create a static variable to hold the hash of the updated dataset
        self.dataset_hash = None
        # Create an IPLD mapper
        ipld_ds_mapper = self.ipld_mapper()
        ipld_ds_mapper.set_root(self.latest_hash())
        original_times = set(original_dataset.time.values)
        if type(update_dataset.time.values) == np.datetime64:  # cannot perform iterative (set) operations on a single numpy.datetime64 value
            update_times = set([update_dataset.time.values])
        else: # many values will come as an iterable numpy.ndarray
            update_times = set(update_dataset.time.values)
        insert_times = sorted(update_times.intersection(original_times))
        append_times = sorted(update_times - original_times)

        return insert_times, append_times, ipld_ds_mapper


    def update_parse_operations(self, 
            original_dataset: xr.Dataset, 
            update_dataset: xr.Dataset, 
            insert_times: list, 
            append_times: list, 
            ipld_ds_mapper: ipldstore.IPLDStore):
        """
        An enclosing method triggering insert and/or append operations based on the presence of valid records for either.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        insert_times : list
            Datetimes corresponding to existing records to be replaced in the original dataset
        append_times : list
            Datetimes corresponding to all new records to append to the original dataset
        ipld_ds_mapper : ipldstore.IPLDStore
            An IPLDStore Mapper object linking the Zarr to a CID

        """
        original_chunks = {dim : val_tuple[0] for dim, val_tuple in original_dataset.chunks.items()}
        # First write out updates to existing data using the 'region=' command...
        if len(insert_times) > 0:
            self.insert_into_dataset(original_dataset, update_dataset, insert_times, original_chunks, ipld_ds_mapper)
        else:
            self.info(f"No modified records to insert into original zarr")
        # ...then write new data (appends) using the 'append_dim=' command
        if len(append_times) > 0:
            self.append_to_dataset(update_dataset, append_times, original_chunks, ipld_ds_mapper)
        else:
            self.info(f"No new records to append to original zarr")
        if not self.dataset_hash:
            raise ValueError(f"Update_zarr called but no new records found to insert or append to original zarr.")

        # Regenerate the STAC Item and update the relevant Collection entry with the final hash
        self.finalize_update_metadata()


    def insert_into_dataset(self, 
            original_dataset: xr.Dataset, 
            update_dataset: xr.Dataset, 
            insert_times: list, 
            original_chunks: list, 
            ipld_ds_mapper: ipldstore.IPLDStore):
        """
        Insert new records to an existing dataset along its time dimension using the `append_dim=` flag.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing xr.Dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        insert_times : list
            Datetimes corresponding to existing records to be replaced in the original dataset
        originaL_chunks : dict
            A Dict containing the dimension:size parameters for the original dataset
        ipld_ds_mapper : ipldstore.IPLDStore
            An IPLDStore Mapper object linking the Zarr to a CID

        """
        insert_dataset = self.prep_update_dataset(update_dataset, insert_times, original_chunks)
        date_ranges, regions = self.calculate_update_time_ranges(original_dataset, insert_dataset)
        for dates, region in zip(date_ranges, regions):
            insert_slice = insert_dataset.sel(time=slice(dates[0], dates[1]))
            insert_slice.drop(self.common_vars[:2]).\
                to_zarr(ipld_ds_mapper, consolidated=True, region={"time": slice(region[0], region[1])})
        self.info(f"Inserted records for {len(insert_dataset.time.values)} times from {len(regions)} date range(s) to original zarr")
        # Write inserted dataset hash to heads file
        self.dataset_hash = str(ipld_ds_mapper.freeze())


    def append_to_dataset(self, 
            update_dataset: xr.Dataset, 
            append_times: list, 
            original_chunks: dict, 
            ipld_ds_mapper: ipldstore.IPLDStore):
        """
        Append new records to an existing dataset along its time dimension using the `append_dim=` flag.

        Parameters
        ----------
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        append_times : list
            Datetimes corresponding to all new records to append to the original dataset
        originaL_chunks : dict
            The dimension:size parameters for the original dataset 
        ipld_ds_mapper : ipldstore.IPLDStore
            An IPLDStore Mapper object linking the Zarr to a CID

        """
        append_dataset = self.prep_update_dataset(update_dataset, append_times, original_chunks)
        append_dataset.to_zarr(ipld_ds_mapper, consolidated=True, append_dim="time")
        self.info(f"Appended records for {len(append_dataset.time.values)} datetimes to original zarr")
        # Write appended dataset hash to heads file
        self.dataset_hash = str(ipld_ds_mapper.freeze())


    def prep_update_dataset(self, 
            update_dataset: xr.Dataset, 
            time_filter_vals: list, 
            new_chunks: dict) -> xr.Dataset:
        """
        Select out and format time ranges you wish to insert or append into the original dataset based on specified time range(s) and chunks

        Parameters
        ----------
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records
        time_filter_vals : list
            Datetimes corresponding to all new records to insert or append
        new_chunks : dict
            A Dict containing the dimension:size parameters for the original dataset 

        Returns
        -------
        update_dataset : xr.Dataset
            An xr.Dataset filtered to only the time values in `time_filter_vals`, with correct metadata

        """
         # Xarray will automatically drop dimensions of size 1. A missing time dimension causes all manner of update failures.
        if "time" in update_dataset.dims:
            update_dataset = update_dataset.sel(time=time_filter_vals).transpose("time", self.common_vars[0],self.common_vars[1])
        else:
            update_dataset = update_dataset.expand_dims('time').transpose("time", self.common_vars[0],self.common_vars[1])
        update_dataset = update_dataset.chunk(new_chunks)
        update_dataset = self.set_zarr_metadata(update_dataset)

        print("---------------------Update dataset---------------------")
        print(update_dataset)
        print("--------------------------------------------------------")

        return update_dataset


    def calculate_update_time_ranges(self, 
            original_dataset: xr.Dataset, 
            update_dataset: xr.Dataset) -> tuple[list[datetime.datetime], list[str]]:
        """
        Calculate the start/end dates and index values for contiguous time ranges of updates.
        Used by `update_zarr` to specify the location(s) of updates in a target Zarr dataset.

        Parameters
        ----------
        original_dataset : xr.Dataset
            The existing xr.Dataset
        update_dataset : xr.Dataset
            A dataset containing all updated (insert) and new (append) records

        Returns
        -------
        datetime_ranges : list[datetime.datetime, datetime.datetime]
            A List of (Datetime, Datetime) tuples defining the time ranges of records to insert
        regions_indices: list[int, int]
             A List of (int, int) tuples defining the indices of records to insert

        """
        dataset_time_span = f'1{self.temporal_resolution()[0]}' # NOTE this won't work for months (returns 1 minute), we could define a more precise method with if/else statements if needed.
        complete_time_series = pd.Series(update_dataset.time.values)
        # Define datetime range starts as anything with > 1 unit diff with the previous value, and ends as > 1 unit diff with the following. First/Last will return NAs we must fill.
        starts = ((complete_time_series - complete_time_series.shift(1)).abs().fillna(pd.Timedelta(dataset_time_span*100)) > pd.Timedelta(dataset_time_span))
        ends = ((complete_time_series - complete_time_series.shift(-1)).abs().fillna(pd.Timedelta(dataset_time_span*100)) > pd.Timedelta(dataset_time_span))
        # Filter down the update time series to just the range starts/ends
        insert_datetimes = complete_time_series[starts + ends]
        single_datetime_inserts = complete_time_series[starts & ends]
        # Add single day insert datetimes once more so they can be represented as ranges, then sort for the correct order. Divide the result into a collection of start/end range arrays
        insert_datetimes = np.sort(pd.concat([insert_datetimes, single_datetime_inserts], ignore_index=True).values)
        datetime_ranges = np.array_split(insert_datetimes, (len(insert_datetimes) / 2))
        # Calculate a tuple of the start/end indices for each datetime range
        regions_indices = []
        for date_pair in datetime_ranges:
            start_int = list(original_dataset.time.values).index(original_dataset.sel(time=date_pair[0], method='nearest').time)
            end_int = list(original_dataset.time.values).index(original_dataset.sel(time=date_pair[1], method='nearest').time) + 1
            regions_indices.append((start_int, end_int))

        return datetime_ranges, regions_indices


    def finalize_update_metadata(self):
        """
        Publish updated STAC Item and Collection metadata reflecting the new dataset hash and parameters
        """
        final_dataset = self.zarr_hash_to_dataset(self.dataset_hash)
        self.info(f"IPFS hash is {str(self.dataset_hash)}")
        self.create_stac_item(final_dataset, self.dataset_hash)
        self.update_stac_collection(final_dataset)
